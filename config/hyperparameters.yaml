# =============================================================================
# Game Selection (Hot-Swappable Architecture)
# =============================================================================
# Supported games:
#   pokemon_red - Pokemon Red/Blue (Game Boy, PyBoy emulator)
#   pokemon_emerald - Pokemon Emerald (GBA, mGBA emulator) [TODO: Not yet implemented]
#
# The game will be auto-detected from ROM file extension if not specified.
game: pokemon_red

# =============================================================================
# Environment
# =============================================================================
environment:
  rom_path: src/games/gb/pokemon_red_color.gb  # Will be auto-detected as pokemon_red from .gb extension
  state_path: states/initial.state
  headless: false
  emulation_speed: 5
  action_repeat: 24
  release_frame: 8
  max_steps: 0                # 0 = unlimited, rely on game health rather than episodes

# =============================================================================
# Training
# =============================================================================
training:
  total_steps: 50000
  warmup_steps: 1000
  batch_size: 32
  save_frequency: 5000
  fast_update_frequency: 1
  reward_clip: 15.0            # Match rewards.reward_clip for consistency
  # Note: epsilon is configured in config/brain_configs.yaml under each brain type

# =============================================================================
# Rewards
# =============================================================================
rewards:
  # --- Global progression (BALANCED for stable learning) ---
  step_penalty: 0.0           # No idle penalty
  reward_clip: 15.0            # Wider clip to let amplified battle rewards come through
  catch_pokemon: 10.0          # Moderate reward for catching
  level_up: 12.0               # Clear milestone signal
  heal_success: 4.0            # Encourage survival
  heal_target: 0.85
  level_reward_scale: 0.8      # Moderate scaling

  # --- Intrinsic motivation (DISABLED) ---
  state_novelty_bonus: 0.0
  action_diversity_bonus: 0.0
  progress_consistency_bonus: 0.0

  # --- Exploration / Navigation (BALANCED) ---
  new_tile: 0.5                # Small but consistent exploration reward
  new_map: 5.0                 # Clear signal for map discovery
  nav_new_connection: 6.0      # Finding new paths
  nav_connection_edge: 1.0     # Edge bonus
  nav_warp: 3.0                # Warp bonus
  nav_warp_proximity: 0.1      # Small shaping towards unexplored warps
  
  # Movement penalties (only after repeated failures)
  wall_bump: -0.5             # Mild penalty per bump (accumulates if stuck)
  stale_penalty: -0.1          # Penalty for being completely stuck
  stale_threshold: 50          # Steps before stale penalty kicks in
  
  # Goal-based rewards (only active when using LLM/heuristic strategies)
  goal_bonus:
    explore:
      map_match: 3.0           # Reaching target map
      coordinate_match: 5.0    # Reaching exact target

    train:
      battle_complete: 5.0     # Completing training battles

    survive:
      hp_recovered: 2.0        # Healing after battles

    menu:
      menu_reached: 0.0        # No reward for just reaching menus
      cursor_match: 0.3        # Small reward for cursor targeting

  # --- Battle shaping (Balanced rewards) ---
  battle_tick: -0.02            # Stronger time pressure so battles register
  battle_damage: 50.0           # 10x louder signal for making progress
  battle_damage_taken: -3.0     # 10x louder penalty for taking damage
  battle_win: 80.0              # 10x clearer victory signal
  battle_loss: -30.0            # 10x clearer loss penalty
  battle_hp_high_threshold: 0.0 # Threshold for HP maintenance bonus
  battle_hp_high_bonus: 0.01     # Small bonus for staying healthy
  max_battle_reward_per_battle: 150.0   # Higher cap to match amplified rewards
  min_battle_penalty_per_battle: -100.0 # Cap total penalties per battle

  # --- Menu handling ---
  menu:
    opened_menu: -0.03         # Tiny cost to discourage menu spam
    cursor_move: -0.001        # Very small cost for cursor movement
    cursor_on_target: 0.0      # Don't reward hovering
    correct_target: 0.5        # Small reward for selection
    inactive_penalty: 0.0      # DISABLED: Was penalizing normal exploration
    close_penalty: 0.0
    menu_timeout_penalty: -0.08 # Mild penalty for staying too long in menus

  # --- Repetitive action penalties (REDUCED - single channel, linear scaling) ---
  switch_active_pokemon_penalty: -0.4   # Base penalty, scales linearly up to 3x
  menu_cycling_penalty: -0.1            # Mild cycling penalty
  run_from_trainer_penalty: -1.0        # Base penalty, scales linearly up to 3x

# =============================================================================
# Director
# =============================================================================
director:
  learning_rate: 0.0003
  vision_learning_rate: 0.0001
  update_interval: 128
  num_specialists: 3
  router_hidden_dim: 64

  graph:
    max_nodes: 5000
    downsample_size: 8
    quantization_step: 32

agent:
  learning_rate: 0.0001    # Slightly higher for faster learning
  gamma: 0.99              # Lower for more stable Q-values
  allowed_actions: [0, 1, 2, 3, 4, 5]  # Arrows, A, B
  buffer_capacity: 100000  # Larger buffer for better sample diversity
  min_buffer_size: 2000    # Higher threshold before training starts

goal_llm:
  enabled: True
  api_url: http://localhost:11434/api/chat
  model: poke-llm
  timeout: 50.0
  debug: True

# =============================================================================
# Strategy Configuration (NEW - Pluggable Architecture)
# =============================================================================
# These control goal-setting and reward calculation strategies
# Leave commented to use auto-detection based on goal_llm.enabled
# Uncomment to override manually

# goal_strategy: "llm"        # Options: "llm", "heuristic", "none"
# reward_strategy: "goal_aware"  # Options: "goal_aware", "base", "hybrid"

# Strategy Presets (use with train_refactored.py --strategy <preset>):
#   llm:       Full LLM-based goal-setting with completion rewards
#   heuristic: Hand-crafted heuristic goals with completion rewards
#   reactive:  Pure reactive RL - no goals, no goal rewards (fixes bug!)
#   hybrid:    Heuristic goals with directional shaping only

router_pretrain_path: checkpoints/director_router_pretrained.pth

saves_dir: saves

# =============================================================================
# Debug
# =============================================================================
debug: False
