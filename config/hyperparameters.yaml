# =============================================================================
# Game Selection (Hot-Swappable Architecture)
# =============================================================================
# Supported games:
#   pokemon_red - Pokemon Red/Blue (Game Boy, PyBoy emulator)
#   pokemon_emerald - Pokemon Emerald (GBA, mGBA emulator) [TODO: Not yet implemented]
#
# The game will be auto-detected from ROM file extension if not specified.
game: pokemon_red

# =============================================================================
# Environment
# =============================================================================
environment:
  rom_path: src/games/gb/pokemon_red_color.gb  # Will be auto-detected as pokemon_red from .gb extension
  state_path: states/initial.state
  headless: false
  emulation_speed: 5
  action_repeat: 24
  release_frame: 8
  max_steps: 2048

# =============================================================================
# Training
# =============================================================================
training:
  total_steps: 50000
  warmup_steps: 1000
  batch_size: 32
  save_frequency: 2000
  fast_update_frequency: 4
  reward_clip: 5.0
  epsilon:
    start: 0.9
    end: 0.05
    decay: 50000

# =============================================================================
# Rewards
# =============================================================================
rewards:
  # --- Global progression ---
  step_penalty: -0.001          # Increased slightly to pressure speed
  reward_clip: 10.0            # Lower cap to prevent explosions
  catch_pokemon: 15.0          # Buffed to make it worth more than walking
  level_up: 15.0
  heal_success: 5.0
  heal_target: 0.85
  level_reward_scale: 1.0

  # --- Exploration / Navigation ---
  new_tile: 0.5                # Increased: Exploration > Existing
  new_map: 5.0
  nav_new_connection: 6.0
  nav_connection_edge: 1.0
  nav_warp: 2.5
  
  # CRITICAL FIXES:
  wall_bump: -0.2              # Punishment
  stale_penalty: -0.5
  stale_threshold: 100
  
  # direction_follow_bonus: 0.75  <-- DELETE THIS or Change to 0.0
  # Instead, only reward reaching the specific target tile:
  goal_bonus:
    explore:
      map_match: 6.0
      coordinate_match: 10.0   # Big payout for ARRIVING, not for moving

    train:
      battle_complete: 12.0    # Reward for completing training battles

    survive:
      hp_recovered: 3.0        # Reward for healing after battles

    menu:
      menu_reached: 2.0        # Reward for reaching target menu
      cursor_match: 1.0        # Reward for cursor on target

  # --- Battle shaping ---
  battle_tick: -0.004           # NEGATIVE. Battles cost energy/time. Win fast!
  battle_damage: 4.0            # Increased from 2.5 - more salient progress signal
  battle_damage_taken: -1.0     # Increased from -0.5 - stronger aversion to damage
  battle_win: 25.0              # Increased from 20.0 - victory more valuable
  battle_loss: -15.0            # Increased from -10.0 - stronger deterrent
  battle_hp_high_threshold: 0.8 # Threshold for HP maintenance bonus
  battle_hp_high_bonus: 0.5     # Reward for maintaining high HP in battle

  # --- Menu handling ---
  menu:
    opened_menu: -0.1          # Opening menu costs small amount (prevents spam)
    cursor_move: -0.001         # Moving cursor costs small amount (efficiency)
    cursor_on_target: 0.0      # Don't reward hovering
    correct_target: 5.0        # Reward only the SELECTION
    inactive_penalty: -0.1
    close_penalty: 0.0

  # --- Repetitive action penalties ---
  switch_active_pokemon_penalty: -2.5  # Penalty for trying to switch to active Pokemon (escalates with repetition)
  menu_cycling_penalty: -1.0           # Penalty for oscillating between menu options (SWITCH->STATS->SWITCH)

# =============================================================================
# Director
# =============================================================================
director:
  learning_rate: 0.0003
  vision_learning_rate: 0.0001
  update_interval: 128
  num_specialists: 3
  router_hidden_dim: 64

  graph:
    max_nodes: 5000
    downsample_size: 8
    quantization_step: 32

agent:
  learning_rate: 0.0003
  gamma: 0.99  # Increased from 0.98 for better long-term credit assignment (battles)
  allowed_actions: [0, 1, 2, 3, 4, 5]  # Arrows, A, B

goal_llm:
  enabled: True
  api_url: http://localhost:11434/api/chat
  model: poke-llm
  timeout: 50.0
  debug: True

# =============================================================================
# Strategy Configuration (NEW - Pluggable Architecture)
# =============================================================================
# These control goal-setting and reward calculation strategies
# Leave commented to use auto-detection based on goal_llm.enabled
# Uncomment to override manually

# goal_strategy: "llm"        # Options: "llm", "heuristic", "none"
# reward_strategy: "goal_aware"  # Options: "goal_aware", "base", "hybrid"

# Strategy Presets (use with train_refactored.py --strategy <preset>):
#   llm:       Full LLM-based goal-setting with completion rewards
#   heuristic: Hand-crafted heuristic goals with completion rewards
#   reactive:  Pure reactive RL - no goals, no goal rewards (fixes bug!)
#   hybrid:    Heuristic goals with directional shaping only

router_pretrain_path: checkpoints/director_router_pretrained.pth

saves_dir: saves

# =============================================================================
# Debug
# =============================================================================
debug: False
