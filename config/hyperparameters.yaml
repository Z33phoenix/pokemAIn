# =============================================================================
# Game Selection (Hot-Swappable Architecture)
# =============================================================================
# Supported games:
#   pokemon_red - Pokemon Red/Blue (Game Boy, PyBoy emulator)
#   pokemon_emerald - Pokemon Emerald (GBA, mGBA emulator) [TODO: Not yet implemented]
#
# The game will be auto-detected from ROM file extension if not specified.
game: pokemon_red

# =============================================================================
# Environment
# =============================================================================
environment:
  rom_path: src/games/gb/pokemon_red_color.gb  # Will be auto-detected as pokemon_red from .gb extension
  state_path: states/initial.state
  headless: false
  emulation_speed: 5
  action_repeat: 24
  release_frame: 8
  max_steps: 2048

# =============================================================================
# Training
# =============================================================================
training:
  total_steps: 50000
  warmup_steps: 1000
  batch_size: 32
  save_frequency: 2000
  fast_update_frequency: 4
  reward_clip: 5.0
  epsilon:
    start: 0.9
    end: 0.05
    decay: 50000

# =============================================================================
# Rewards
# =============================================================================
rewards:
  # --- Global ---
  step_penalty: -0.01
  reward_clip: 10.0
  catch_pokemon: 5.0
  level_up: 10.0
  heal_success: 3.0
  heal_target: 0.9

  # --- Navigation shaping ---
  new_tile: 0.2
  new_map: 2.0
  nav_new_connection: 3.0   # Bonus for first traversal of a unique map-to-map edge (warp or boundary)
  nav_connection_edge: 0.5     # Bonus for reaching an edge that connects to another map
  nav_connection_discovery: 0.5  # Bonus once per map for having connection data available
  nav_warp: 1.0                # Bonus for stepping onto a warp tile (door/stair transition)
  nav_warp_proximity: 0.05     # Small shaping toward known warp tiles (scaled by Manhattan distance)
  wall_bump: -0.2
  stale_penalty: -0.3
  stale_threshold: 120
  direction_follow_bonus: 0.5  # Scales reward for moving along goal_vector direction (dot product)

# =============================================================================
# Director
# =============================================================================
director:
  learning_rate: 0.0003
  vision_learning_rate: 0.0001
  update_interval: 128
  num_specialists: 3
  router_hidden_dim: 64

  graph:
    max_nodes: 5000
    downsample_size: 8
    quantization_step: 32

agent:
  learning_rate: 0.0003
  gamma: 0.98
  allowed_actions: [0, 1, 2, 3, 4, 5]  # Arrows, A, B

goal_llm:
  enabled: False
  api_url: http://localhost:11434/api/chat
  model: pokemon-goal
  timeout: 50.0
  debug: False

# =============================================================================
# Strategy Configuration (NEW - Pluggable Architecture)
# =============================================================================
# These control goal-setting and reward calculation strategies
# Leave commented to use auto-detection based on goal_llm.enabled
# Uncomment to override manually

# goal_strategy: "llm"        # Options: "llm", "heuristic", "none"
# reward_strategy: "goal_aware"  # Options: "goal_aware", "base", "hybrid"

# Strategy Presets (use with train_refactored.py --strategy <preset>):
#   llm:       Full LLM-based goal-setting with completion rewards
#   heuristic: Hand-crafted heuristic goals with completion rewards
#   reactive:  Pure reactive RL - no goals, no goal rewards (fixes bug!)
#   hybrid:    Heuristic goals with directional shaping only

router_pretrain_path: checkpoints/director_router_pretrained.pth

saves_dir: saves

# =============================================================================
# Debug
# =============================================================================
debug: True
