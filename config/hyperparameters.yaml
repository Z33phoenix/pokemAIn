# =============================================================================
# Game Selection (Hot-Swappable Architecture)
# =============================================================================
# Supported games:
#   pokemon_red - Pokemon Red/Blue (Game Boy, PyBoy emulator)
#   pokemon_emerald - Pokemon Emerald (GBA, mGBA emulator) [TODO: Not yet implemented]
#
# The game will be auto-detected from ROM file extension if not specified.
game: pokemon_red

# =============================================================================
# Environment
# =============================================================================
environment:
  rom_path: src/games/gb/pokemon_red_color.gb  # Will be auto-detected as pokemon_red from .gb extension
  state_path: states/initial.state
  headless: false
  emulation_speed: 5
  action_repeat: 24
  release_frame: 8
  max_steps: 2048

# =============================================================================
# Training
# =============================================================================
training:
  total_steps: 50000
  warmup_steps: 1000
  batch_size: 32
  save_frequency: 2000
  fast_update_frequency: 4
  reward_clip: 5.0
  epsilon:
    start: 1.0  # Start with full exploration
    end: 0.02  # Lower final epsilon for sparse rewards
    decay: 150000  # Much slower decay for Pokemon's complexity

# =============================================================================
# Rewards
# =============================================================================
rewards:
  # --- Global progression ---
  step_penalty: -0.002         # Increased to prevent idle farming
  reward_clip: 10.0            # Lower cap to prevent explosions
  catch_pokemon: 20.0          # Higher value for rare positive events
  level_up: 25.0               # Significantly boosted for major milestones
  heal_success: 8.0            # Increased to encourage survival behavior
  heal_target: 0.85
  level_reward_scale: 1.5      # Increased scaling for level progression
  
  # --- Intrinsic motivation for sparse rewards (DISABLED - causing exploitation) ---
  state_novelty_bonus: 0.0     # DISABLED: Was causing battle start screen farming
  action_diversity_bonus: 0.0  # DISABLED: Not implemented, preventing confusion
  progress_consistency_bonus: 0.0  # DISABLED: Was rewarding non-progress

  # --- Exploration / Navigation ---
  new_tile: 1.0                # Doubled for better exploration signal
  new_map: 10.0                # Significantly increased for major discoveries
  nav_new_connection: 12.0     # Doubled for finding new paths
  nav_connection_edge: 2.0     # Increased edge bonus
  nav_warp: 5.0                # Doubled warp bonus
  nav_warp_proximity: 0.2      # Small shaping towards unexplored warps
  
  # CRITICAL FIXES:
  wall_bump: -0.2              # Punishment
  stale_penalty: -0.5
  stale_threshold: 100
  
  # direction_follow_bonus: 0.75  <-- DELETE THIS or Change to 0.0
  # Instead, only reward reaching the specific target tile:
  goal_bonus:
    explore:
      map_match: 6.0
      coordinate_match: 10.0   # Big payout for ARRIVING, not for moving

    train:
      battle_complete: 12.0    # Reward for completing training battles

    survive:
      hp_recovered: 3.0        # Reward for healing after battles

    menu:
      menu_reached: 0.0        # REMOVED: No reward for just reaching menus
      cursor_match: 0.5        # Reduced reward for cursor targeting

  # --- Battle shaping (High damage rewards) ---
  battle_tick: -0.005           # Reduced penalty during battles to not overwhelm damage rewards
  battle_damage: 15.0           # MUCH higher - make damage clearly rewarding
  battle_damage_taken: -0.5     # Mild penalty for taking damage
  battle_win: 20.0              # Increased for major victories
  battle_loss: -5.0             # Milder loss penalty
  battle_hp_high_threshold: 0.8 # Threshold for HP maintenance bonus
  battle_hp_high_bonus: 0.2     # Reduced from 0.5
  max_battle_reward_per_battle: 40.0   # Higher cap for increased damage rewards
  min_battle_penalty_per_battle: -20.0 # NEW: Cap total penalties per battle

  # --- Menu handling ---
  menu:
    opened_menu: -0.5          # Higher cost to discourage menu spam
    cursor_move: -0.01         # Higher cost for cursor movement
    cursor_on_target: 0.0      # Don't reward hovering
    correct_target: 2.0        # Reduced reward for selection
    inactive_penalty: -0.1
    close_penalty: 0.0
    menu_timeout_penalty: -5.0 # NEW: Penalty for staying too long in menus

  # --- Repetitive action penalties ---
  switch_active_pokemon_penalty: -15.0  # EVEN MORE SEVERE penalty 
  menu_cycling_penalty: -8.0            # MUCH STRONGER cycling penalty
  run_from_trainer_penalty: -12.0       # SEVERE penalty for trying to run from trainer battles

# =============================================================================
# Director
# =============================================================================
director:
  learning_rate: 0.0003
  vision_learning_rate: 0.0001
  update_interval: 128
  num_specialists: 3
  router_hidden_dim: 64

  graph:
    max_nodes: 5000
    downsample_size: 8
    quantization_step: 32

agent:
  learning_rate: 0.00005  # Reduced for stable convergence in sparse environment
  gamma: 0.995  # Higher for better long-term credit assignment in Pokemon
  allowed_actions: [0, 1, 2, 3, 4, 5]  # Arrows, A, B
  buffer_capacity: 100000  # Larger buffer for better sample diversity
  min_buffer_size: 2000  # Higher threshold before training starts

goal_llm:
  enabled: True
  api_url: http://localhost:11434/api/chat
  model: poke-llm
  timeout: 50.0
  debug: True

# =============================================================================
# Strategy Configuration (NEW - Pluggable Architecture)
# =============================================================================
# These control goal-setting and reward calculation strategies
# Leave commented to use auto-detection based on goal_llm.enabled
# Uncomment to override manually

# goal_strategy: "llm"        # Options: "llm", "heuristic", "none"
# reward_strategy: "goal_aware"  # Options: "goal_aware", "base", "hybrid"

# Strategy Presets (use with train_refactored.py --strategy <preset>):
#   llm:       Full LLM-based goal-setting with completion rewards
#   heuristic: Hand-crafted heuristic goals with completion rewards
#   reactive:  Pure reactive RL - no goals, no goal rewards (fixes bug!)
#   hybrid:    Heuristic goals with directional shaping only

router_pretrain_path: checkpoints/director_router_pretrained.pth

saves_dir: saves

# =============================================================================
# Debug
# =============================================================================
debug: False
